import streamlit as st
import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import joblib
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, confusion_matrix
)

st.set_page_config(
    page_title="Stroke Prediction - Data Mining",
    layout="wide"
)

BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data" / "processed"
FIG_DIR = BASE_DIR / "src" / "preprocess"
MODEL_DIR = BASE_DIR / "src" / "models"
RESULTS_DIR = BASE_DIR / "data" / "results"

@st.cache_data
def load_logistic_comparison():
    comp_path = RESULTS_DIR / "logistic_comparison.csv"
    comp_df = pd.read_csv(comp_path)

    raw_cm_path = RESULTS_DIR / "confusion_matrix_logistic_raw.csv"
    proc_cm_path = RESULTS_DIR / "confusion_matrix_logistic_processed.csv"

    raw_cm = pd.read_csv(raw_cm_path, index_col=0)
    proc_cm = pd.read_csv(proc_cm_path, index_col=0)

    return comp_df, raw_cm, proc_cm

@st.cache_data
def load_logistic_results():
    raw_path = RESULTS_DIR / "logistic_regression_raw_metrics.csv"
    proc_path = RESULTS_DIR / "logistic_regression_processed_metrics.csv"
    comp_path = RESULTS_DIR / "comparison_logistic_models.csv"

    raw_df = pd.read_csv(raw_path)
    proc_df = pd.read_csv(proc_path)
    comp_df = pd.read_csv(comp_path)

    return raw_df, proc_df, comp_df

@st.cache_data
def load_xgb_results():
    path = RESULTS_DIR / "xgboost_variants_metrics.csv"
    if not path.exists():
        return None
    dfXG = pd.read_csv(path)
    return dfXG

@st.cache_resource
def load_xgb_model():
    return joblib.load(RESULTS_DIR / "xgb.pkl")

@st.cache_resource
def load_lgbm_model():
    model_path = RESULTS_DIR / "lightgbm.pkl"
    return joblib.load(model_path)


@st.cache_data
def load_lgbm_best_info():
    path = RESULTS_DIR / "lightgbm.csv"
    if not path.exists():
        return None
    return pd.read_csv(path)


@st.cache_data
def load_lgbm_confusion():
    path = RESULTS_DIR / "confusion_matrix_lightgbm.csv"
    if not path.exists():
        return None
    return pd.read_csv(path, index_col=0)
    
@st.cache_data
def load_validation_data():
    val_path = DATA_DIR / "validation.csv"
    df_val = pd.read_csv(val_path)
    X_val = df_val.drop(columns=["stroke"])
    y_val = df_val["stroke"]
    return X_val, y_val

st.sidebar.header("üìö N·ªôi dung")
page = st.sidebar.radio(
    "",
    ["1. Gi·ªõi thi·ªáu v√† x·ª≠ l√Ω d·ªØ li·ªáu",
    "2. M√¥ h√¨nh XGBoost",
    "3. M√¥ h√¨nh LightGBM",
    "4. So s√°nh 2 m√¥ h√¨nh"]
)

st.title("B√†i t·∫≠p l·ªõp m√¥n Khai ph√° d·ªØ li·ªáu: D·ª± √°n d·ª± ƒëo√°n nguy c∆° ƒë·ªôt qu·ªµ")

if page.startswith("1."):
    st.markdown("### Ph·∫ßn 1 - Gi·ªõi thi·ªáu, m√¥ t·∫£ v√† x·ª≠ l√Ω d·ªØ li·ªáu")

    @st.cache_data
    def load_processed_data():
        dfs = {}
        for name, filename in [
            ("train_balanced", "train_balanced.csv"),
            ("train_scaled", "train_scaled.csv"),
            ("validation", "validation.csv"),
            ("test", "test.csv"),
        ]:
            path = DATA_DIR / filename
            if path.exists():
                dfs[name] = pd.read_csv(path)
            else:
                dfs[name] = None
        return dfs

    def load_raw_data():
        data_path = Path("data/raw/healthcare-dataset-stroke-data.csv")
        df = pd.read_csv(data_path)
        return df

    df = load_raw_data()
    dfs = load_processed_data()
    train_balanced = dfs["train_balanced"]
    train_scaled = dfs["train_scaled"]
    val_df = dfs["validation"]
    test_df = dfs["test"]

    data_info_path = DATA_DIR / "data_info.txt"
    feature_names_path = DATA_DIR / "feature_names.txt"

    data_info_text = data_info_path.read_text(encoding="utf-8") if data_info_path.exists() else None
    feature_names = (
        feature_names_path.read_text(encoding="utf-8").splitlines()
        if feature_names_path.exists()
        else None
    )

    st.subheader("1Ô∏è. T·ªïng quan th√¥ng tin d·ªØ li·ªáu")

    col1, col2 = st.columns(2)

    with col1:
        st.write("**K√≠ch th∆∞·ªõc d·ªØ li·ªáu:**")
        st.write(f"- S·ªë d√≤ng: **{df.shape[0]:,}**")
        st.write(f"- S·ªë c·ªôt: **{df.shape[1]}**")

    with col2:
        st.write("**C√°c c·ªôt trong dataset:**")
        st.write(list(df.columns))

    st.markdown("**5 d√≤ng ƒë·∫ßu ti√™n:**")
    st.dataframe(df.head())

    st.subheader("2Ô∏è. Ph√¢n t√≠ch missing values")

    missing_info = pd.DataFrame({
        "Column": df.columns,
        "Missing_Count": df.isnull().sum(),
        "Missing_Percentage": (df.isnull().sum() / len(df) * 100).round(2),
        "Data_Type": df.dtypes.astype(str)
    })

    missing_info = missing_info[missing_info["Missing_Count"] > 0] \
        .sort_values("Missing_Percentage", ascending=False)

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**Ki·ªÉm tra gi√° tr·ªã `'N/A'` d·∫°ng s·ªë:**")
        if missing_info.empty:
            st.write("‚úÖ Kh√¥ng c√≥ c·ªôt n√†o b·ªã thi·∫øu.")
        else:
            st.dataframe(missing_info)

    # Check 'N/A' d·∫°ng string trong c√°c c·ªôt object
    with col2:
        st.markdown("**Ki·ªÉm tra gi√° tr·ªã `'N/A'` d·∫°ng string:**")
        na_string_rows = []
        for col in df.columns:
            if df[col].dtype == "object":
                na_count = (df[col] == "N/A").sum()
                if na_count > 0:
                    na_string_rows.append({
                        "Column": col,
                        "N/A_Count": na_count,
                        "N/A_Percentage": na_count / len(df) * 100
                    })

        if len(na_string_rows) == 0:
            st.write("‚úÖ Kh√¥ng c√≥ gi√° tr·ªã `'N/A'` d·∫°ng string trong c√°c c·ªôt category.")
        else:
            na_string_df = pd.DataFrame(na_string_rows)
            na_string_df["N/A_Percentage"] = na_string_df["N/A_Percentage"].round(2)
            st.dataframe(na_string_df)

    st.subheader("3. Ph√¢n t√≠ch outliers")

    with st.expander("Xem th·ªëng k√™ outlier"):
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        for col in ["id", "stroke"]:
            if col in numeric_cols:
                numeric_cols.remove(col)

        outlier_rows = []
        for col in numeric_cols:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower = Q1 - 1.5 * IQR
            upper = Q3 + 1.5 * IQR

            mask = (df[col] < lower) | (df[col] > upper)
            outlier_count = mask.sum()
            if outlier_count > 0:
                outlier_rows.append({
                    "Column": col,
                    "Outlier_Count": outlier_count,
                    "Outlier_Percentage": outlier_count / len(df) * 100,
                    "Lower_Bound": round(lower, 2),
                    "Upper_Bound": round(upper, 2),
                    "Min": round(df[col].min(), 2),
                    "Max": round(df[col].max(), 2),
                })

        if len(outlier_rows) == 0:
            st.write("‚úÖ Kh√¥ng ph√°t hi·ªán outlier theo IQR cho c√°c c·ªôt numeric.")
        else:
            outlier_df = pd.DataFrame(outlier_rows)
            outlier_df["Outlier_Percentage"] = outlier_df["Outlier_Percentage"].round(2)
            st.dataframe(outlier_df)

    st.subheader("4. Ki·ªÉm tra d·ªØ li·ªáu tr√πng l·∫∑p v√† ph√¢n ph·ªëi target")

    duplicates = df.duplicated().sum()
    st.write(f"**S·ªë d√≤ng tr√πng l·∫∑p ho√†n to√†n:** `{duplicates}`")

    # Ph√¢n ph·ªëi target stroke
    if "stroke" in df.columns:
        stroke_dist = df["stroke"].value_counts().sort_index()
        total = len(df)

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("**B·∫£ng ph√¢n ph·ªëi target (`stroke`):**")
            dist_table = pd.DataFrame({
                "stroke": stroke_dist.index,
                "count": stroke_dist.values,
                "percentage": (stroke_dist.values / total * 100).round(2)
            })
            st.dataframe(dist_table)

            if 0 in stroke_dist and 1 in stroke_dist:
                imbalance_ratio = stroke_dist[0] / stroke_dist[1]
                st.write(f"**Imbalance ratio:** ~ `{imbalance_ratio:.2f} : 1`")

        with col2:
            st.markdown("**Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi target:**")
            st.bar_chart(stroke_dist)
    else:
        st.error("Kh√¥ng t√¨m th·∫•y c·ªôt `stroke` trong d·ªØ li·ªáu.")

    st.subheader("5. C√°c bi·ªÉu ƒë·ªì ph√¢n t√≠ch d·ªØ li·ªáu")

    corr_img = FIG_DIR / "correlation_matrix.png"
    dist_img = FIG_DIR / "distribution_analysis.png"
    rel_img = FIG_DIR / "feature_target_relationship.png"

    if dist_img.exists():
        st.markdown("**Ph√¢n t√≠ch ph√¢n ph·ªëi c√°c bi·∫øn (distribution analysis):**")
        st.image(dist_img, use_container_width=True)
    else:
        st.warning("Kh√¥ng t√¨m th·∫•y `distribution_analysis.png`.")

    st.write("")

    col1, col2 = st.columns(2)

    with col1:
        if corr_img.exists():
            st.markdown("**Ma tr·∫≠n t∆∞∆°ng quan (correlation matrix):**")
            st.image(corr_img, use_container_width=True)
        else:
            st.warning("Kh√¥ng t√¨m th·∫•y `correlation_matrix.png`.")

    with col2:
        if rel_img.exists():
            st.markdown("**M·ªëi quan h·ªá gi·ªØa feature v√† target (feature-target relationship):**")
            st.image(rel_img, use_container_width=True)
        else:
            st.warning("Kh√¥ng t√¨m th·∫•y `feature_target_relationship.png`.")

    st.subheader("6. Ti·∫øn h√†nh x·ª≠ l√Ω ti·ªÅn d·ªØ li·ªáu")

    st.markdown("""
    - **L√†m s·∫°ch d·ªØ li·ªáu:**
        - X·ª≠ l√Ω gi√° tr·ªã thi·∫øu, ƒë·∫∑c bi·ªát ·ªü c·ªôt `bmi` (chuy·ªÉn `'N/A'` ‚Üí missing, sau ƒë√≥ fill).
        - Lo·∫°i b·ªè/gi·ªõi h·∫°n c√°c gi√° tr·ªã b·∫•t th∆∞·ªùng (outlier) theo c√°c rule ƒë√£ thi·∫øt k·∫ø.
        - B·ªè c·ªôt kh√¥ng h·ªØu √≠ch nh∆∞ `id`.

    - **Bi·∫øn ƒë·ªïi & t·∫°o th√™m feature:**
        - T·∫°o c√°c nh√≥m tu·ªïi, nh√≥m BMI, nh√≥m glucose.
        - M√£ h√≥a c√°c bi·∫øn ph√¢n lo·∫°i (one-hot encoding) cho c√°c c·ªôt nh∆∞ `gender`, `smoking_status`, c√°c c·ªôt nh√≥m, v.v.
        - Thu ƒë∆∞·ª£c t·∫≠p feature cu·ªëi c√πng v·ªõi **26 feature**.

    - **Chia t·∫≠p & chu·∫©n h√≥a:**
        - Chia d·ªØ li·ªáu th√†nh Train / Validation / Test.
        - Chu·∫©n h√≥a feature (scaling).
        - √Åp d·ª•ng **SMOTE** tr√™n t·∫≠p Train ƒë·ªÉ x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng l·ªõp (stroke = 0/1).
    """)

    st.subheader("7. C√°c t·∫≠p d·ªØ li·ªáu sau x·ª≠ l√Ω")

    if train_scaled is not None:
        rows, cols = train_scaled.shape
        st.markdown(f"**Train (scaled):** `({rows}, {cols})`")
        st.dataframe(train_scaled.head())
    else:
        st.warning("Kh√¥ng t√¨m th·∫•y `train_scaled.csv`.")

    st.write("")

    if train_balanced is not None:
        rows, cols = train_balanced.shape
        st.markdown(f"**Train (balanced sau SMOTE):** `({rows}, {cols})`")
        st.dataframe(train_balanced.head())
    else:
        st.warning("Kh√¥ng t√¨m th·∫•y `train_balanced.csv`.")

    st.write("")

    if val_df is not None:
        rows, cols = val_df.shape
        st.markdown(f"**Validation:** `({rows}, {cols})`")
        st.dataframe(val_df.head())
    else:
        st.warning("Kh√¥ng t√¨m th·∫•y `validation.csv`.")

    st.write("")

    if test_df is not None:
        rows, cols = test_df.shape
        st.markdown(f"**Test:** `({rows}, {cols})`")
        st.dataframe(test_df.head())
    else:
        st.warning("Kh√¥ng t√¨m th·∫•y `test.csv`.")

    st.subheader("8. ·∫¢nh h∆∞·ªüng c·ªßa vi·ªác ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu")

    comp_df, raw_cm, proc_cm = load_logistic_comparison()

    st.markdown("#### 8.1. Confusion matrix c·ªßa 2 tr∆∞·ªùng h·ª£p")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**RAW data (d·ªØ li·ªáu th√¥):**")
        fig, ax = plt.subplots()
        sns.heatmap(raw_cm, annot=True, fmt="d", cmap="Blues", ax=ax)
        ax.set_title("Confusion Matrix - RAW data")
        ax.set_xlabel("D·ª± ƒëo√°n")
        ax.set_ylabel("Th·ª±c t·∫ø")
        st.pyplot(fig)

    with col2:
        st.markdown("**PROCESSED data (d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω):**")
        fig, ax = plt.subplots()
        sns.heatmap(proc_cm, annot=True, fmt="d", cmap="Greens", ax=ax)
        ax.set_title("Confusion Matrix - PROCESSED data")
        ax.set_xlabel("D·ª± ƒëo√°n")
        ax.set_ylabel("Th·ª±c t·∫ø")
        st.pyplot(fig)

    st.caption("H√†ng = gi√° tr·ªã th·ª±c t·∫ø, c·ªôt = d·ª± ƒëo√°n. L·ªõp 1 l√† b·ªánh nh√¢n b·ªã ƒë·ªôt qu·ªµ.")

    st.markdown("#### 8.2. B·∫£ng so s√°nh t·ªïng qu√°t")

    comp_display = comp_df.copy()
    comp_display.index = ["Raw data", "Processed data"]

    for col in ["precision", "recall", "f1-score", "accuracy", "time_seconds"]:
        comp_display[col] = comp_display[col].astype(float).round(3)

    st.dataframe(comp_display, use_container_width=True)

    st.caption("""
- **precision / recall / f1-score / accuracy**: metric t·ªïng th·ªÉ c·ªßa t·ª´ng model.
- **time_seconds**: th·ªùi gian hu·∫•n luy·ªán + d·ª± ƒëo√°n (x·∫•p x·ªâ).
""")

elif page.startswith("2."):
    st.markdown("### Ph·∫ßn 2 - M√¥ h√¨nh XGBoost")

    st.subheader("1. C·∫•u h√¨nh m√¥ h√¨nh")

    xgb_params = {
        "n_estimators": 500,
        "max_depth": 5,
        "learning_rate": 0.03,
        "scale_pos_weight": "pos_weight = (s·ªë m·∫´u l·ªõp 0 / s·ªë m·∫´u l·ªõp 1)",
        "subsample": 0.8,
        "colsample_bytree": 0.8,
        "random_state": 42,
        "eval_metric": "logloss",
        "n_jobs": -1,
    }

    st.markdown("""
    - Train tr√™n: **train_balanced.csv** (d·ªØ li·ªáu ƒë√£ c√¢n b·∫±ng b·∫±ng SMOTE).
    - Test tr√™n: **test.csv**.
    - D√πng tham s·ªë `scale_pos_weight` ƒë·ªÉ x·ª≠ l√Ω l·ªách l·ªõp.
    """)
    st.json(xgb_params)

    st.subheader("2. K·∫øt qu·∫£ c√°c bi·∫øn th·ªÉ XGBoost (threshold kh√°c nhau)")

    xgb_results = load_xgb_results()
    if xgb_results is None:
        st.warning(
            "Kh√¥ng t√¨m th·∫•y `xgboost_variants_metrics.csv`. "
        )
    else:
        st.write("**B·∫£ng metric:**")

        display_cols = ["model", "variant", "threshold",
                        "precision", "recall", "f1-score",
                        "accuracy", "time_seconds"]
        extra_cols = [c for c in ["F2_1"] if c in xgb_results.columns]
        display_cols += extra_cols
        display_cols = [c for c in display_cols if c in xgb_results.columns]

        styled = (
            xgb_results[display_cols]
            .style.format({
                "threshold": "{:.3f}",
                "precision": "{:.3f}",
                "recall": "{:.3f}",
                "f1-score": "{:.3f}",
                "accuracy": "{:.3f}",
                "time_seconds": "{:.2f}",
                "F2_1": "{:.3f}" if "F2_1" in xgb_results.columns else None,
            })
        )
        st.dataframe(styled, use_container_width=True)
        st.markdown("""
            - **default_0.5**: d√πng ng∆∞·ª°ng m·∫∑c ƒë·ªãnh 0.5 cho x√°c su·∫•t.
            - **F2_opt**: ch·ªçn threshold t·ªëi ∆∞u **F2-score** cho l·ªõp 1 (∆∞u ti√™n Recall h∆°n Precision).
            - **Recall_opt**: ch·ªçn threshold ƒë·ªÉ **Recall c·ªßa l·ªõp 1 cao nh·∫•t**, v·ªõi r√†ng bu·ªôc Precision kh√¥ng qu√° th·∫•p.
        """)
        st.caption("C√°c metric precision / recall / F1-score ƒë∆∞·ª£c t√≠nh cho l·ªõp **1 (stroke = 1)**.")

        # ==== 3. V·∫Ω bi·ªÉu ƒë·ªì so s√°nh gi·ªØa c√°c variant ====
        st.subheader("3. So s√°nh hi·ªáu nƒÉng gi·ªØa c√°c bi·∫øn th·ªÉ")

        metric_options = ["precision", "recall", "f1-score", "accuracy"]
        if "F2_1" in xgb_results.columns:
            metric_options.insert(0, "F2_1")

        metric_to_plot = st.selectbox("Ch·ªçn metric ƒë·ªÉ v·∫Ω:", metric_options)

        plot_df = (
            xgb_results
            .set_index("variant")[metric_to_plot]
            .sort_values(ascending=False)
        )

        st.bar_chart(plot_df)

        # ==== 4. Ch·ªçn 1 bi·∫øn th·ªÉ ƒë·ªÉ highlight ====
        st.subheader("4. Ph√¢n t√≠ch chi ti·∫øt 1 c·∫•u h√¨nh XGBoost")

        variant_names = xgb_results["variant"].unique().tolist()
        chosen_variant = st.selectbox("Ch·ªçn bi·∫øn th·ªÉ:", variant_names)

        row = xgb_results[xgb_results["variant"] == chosen_variant].iloc[0]

        col1, col2 = st.columns(2)
        with col1:
            st.write("**Threshold s·ª≠ d·ª•ng:**", f"`{row['threshold']:.3f}`")
            st.write("**Precision (class 1):**", f"{row['precision']:.3f}")
            st.write("**Recall (class 1):**", f"{row['recall']:.3f}")
            st.write("**F1-score (class 1):**", f"{row['f1-score']:.3f}")

        with col2:
            st.write("**Accuracy:**", f"{row['accuracy']:.3f}")
            st.write("**Th·ªùi gian train + ƒë√°nh gi√° (s):**", f"{row['time_seconds']:.2f}")
            if "F2_1" in row and not pd.isna(row["F2_1"]):
                st.write("**F2-score (class 1):**", f"{row['F2_1']:.3f}")

elif page.startswith("3."):
    st.markdown("### Ph·∫ßn 3 - M√¥ h√¨nh LightGBM")

    # ===== 1. C·∫•u h√¨nh & tuning =====
    st.subheader("1. C·∫•u h√¨nh & qu√° tr√¨nh tuning")

    st.markdown("""
    - Train tr√™n: **train_balanced.csv** (d·ªØ li·ªáu ƒë√£ c√¢n b·∫±ng b·∫±ng SMOTE).
    - Validation tr√™n: **validation.csv**.
    - S·ª≠ d·ª•ng **Grid Search ƒë∆°n gi·∫£n tr√™n validation set** v·ªõi c√°c tham s·ªë:
        - `num_leaves`: [15, 31, 63]
        - `max_depth`: [-1, 7, 11]
        - `learning_rate`: [0.01, 0.05, 0.1]
        - `feature_fraction`: [0.8, 1.0]
        - `bagging_fraction`: [0.8, 1.0]
        - `bagging_freq`: [0, 5]
        - `min_child_samples`: [20, 50]
    - M·ªói t·ªï h·ª£p tham s·ªë:
        - Train `LGBMClassifier` v·ªõi `n_estimators = 5000`, `early_stopping_rounds = 100`.
        - ƒêo **AUC tr√™n validation** v√† ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t.
    """)

    best_info = load_lgbm_best_info()
    if best_info is not None:
        st.markdown("**C√°c tham s·ªë t·ªët nh·∫•t t√¨m ƒë∆∞·ª£c (tr√™n validation):**")
        st.dataframe(best_info, use_container_width=True)
        if "best_auc_valid" in best_info.columns:
            best_auc_value = best_info["best_auc_valid"].iloc[0]
            st.write(f"**Best AUC (validation):** `{best_auc_value:.3f}`")
    else:
        st.warning("Ch∆∞a t√¨m th·∫•y `lightgbm.csv`")

    st.write("---")

    # ===== 2. ƒê√°nh gi√° l·∫°i LightGBM tr√™n validation =====
    st.subheader("2. Hi·ªáu nƒÉng LightGBM tr√™n t·∫≠p validation")

    try:
        lgbm_model = load_lgbm_model()
        X_val, y_val = load_validation_data()
    except Exception as e:
        st.error(f"Kh√¥ng load ƒë∆∞·ª£c model ho·∫∑c d·ªØ li·ªáu validation: {e}")
        st.stop()

    # Predict
    y_proba = lgbm_model.predict_proba(X_val)[:, 1]
    y_pred = lgbm_model.predict(X_val)

    # Metrics
    acc = accuracy_score(y_val, y_pred)
    prec = precision_score(y_val, y_pred, zero_division=0)
    rec = recall_score(y_val, y_pred, zero_division=0)
    f1 = f1_score(y_val, y_pred, zero_division=0)
    auc = roc_auc_score(y_val, y_proba)

    col1, col2 = st.columns(2)
    with col1:
        st.write(f"**Accuracy:** `{acc:.3f}`")
        st.write(f"**Precision (class 1):** `{prec:.3f}`")
        st.write(f"**Recall (class 1):** `{rec:.3f}`")
    with col2:
        st.write(f"**F1-score (class 1):** `{f1:.3f}`")
        st.write(f"**AUC (validation):** `{auc:.3f}`")

    st.caption("C√°c metric precision / recall / F1-score ƒë∆∞·ª£c t√≠nh cho l·ªõp **1 (stroke = 1)**.")

    # ===== 3. Confusion matrix =====
    st.subheader("3. Ma tr·∫≠n nh·∫ßm l·∫´n (Confusion Matrix)")

    cm_df = load_lgbm_confusion()
    if cm_df is None:
        # n·∫øu v√¨ l√Ω do g√¨ ƒë√≥ kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file CSV, t·ª± t√≠nh l·∫°i t·ª´ y_val & y_pred
        cm = confusion_matrix(y_val, y_pred)
        cm_df = pd.DataFrame(
            cm,
            index=["Actual_0", "Actual_1"],
            columns=["Pred_0", "Pred_1"]
        )

    st.markdown("**B·∫£ng confusion matrix:**")
    st.dataframe(cm_df)

    import matplotlib.pyplot as plt

    fig, ax = plt.subplots()
    im = ax.imshow(cm_df.values)

    ax.set_xticks(range(len(cm_df.columns)))
    ax.set_xticklabels(cm_df.columns)
    ax.set_yticks(range(len(cm_df.index)))
    ax.set_yticklabels(cm_df.index)

    for i in range(cm_df.shape[0]):
        for j in range(cm_df.shape[1]):
            ax.text(j, i, cm_df.values[i, j],
                    ha="center", va="center")

    ax.set_title("Confusion Matrix - LightGBM (Validation)")
    st.pyplot(fig)

elif page.startswith("4."):
    st.markdown("### Ph·∫ßn 4 - So s√°nh c√°c m√¥ h√¨nh ML")

    st.subheader("1. B·∫£ng so s√°nh c√°c m√¥ h√¨nh")

    try:
        compare_df = pd.read_csv(RESULTS_DIR / "model_comparison.csv")
        st.dataframe(compare_df, use_container_width=True)
    except Exception as e:
        st.error(f"Kh√¥ng th·ªÉ load b·∫£ng model_comparison.csv: {e}")
        st.stop()

    st.subheader("2. Bi·ªÉu ƒë·ªì so s√°nh c√°c metric")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**Precision**")
        st.image(RESULTS_DIR / "precision_plot.png")

        st.markdown("**F1-score**")
        st.image(RESULTS_DIR / "f1_plot.png")

    with col2:
        st.markdown("**Recall**")
        st.image(RESULTS_DIR / "recall_plot.png")

        st.markdown("**Accuracy**")
        st.image(RESULTS_DIR / "accuracy_plot.png")